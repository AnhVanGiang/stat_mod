---
title: "Assignment 1"
date: "`r Sys.Date()`"
output:
  pdf_document:
    includes:
      in_header: preample.tex
---

```{r setup, include=FALSE}
library(knitr)
library(afex)
library(performance)
library(magrittr)
library(plyr)
library(EnvStats)
hook_output <- knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$out.lines)) {
    n <- as.numeric(n)
    x <- unlist(stringr::str_split(x, "\n"))
    nx <- length(x) 
    x <- x[pmin(n,nx)]
    if(min(n) > 1)  
      x <- c(paste(options$comment, "[...]"), x)
    if(max(n) < nx) 
      x <- c(x, paste(options$comment, "[...]"))
    x <- paste(c(x, "\n"), collapse = "\n")
  }
  hook_output(x, options)
    })
```
## Question 1
### a) 

We have $\hali = \frac{1}{n} \sum_{j=1}^{n} Y_{ij}$ so 
    \[ \ex[\hali] = \frac{1}{n} \sum_{j=1}^{n} \ex[Y_{ij}] =  \frac{1}{n} \sum_{j=1}^{n} \alpha_i = \alpha_i.  \]
    Thus $\hali$ is an ubiased estimator. 

### b) 

We have
    \begin{align*}
        \var(\hali) &= \var \left(\frac{1}{n} \sum_{j=1}^{n} Y_{ij} \right) \\
        &= \frac{1}{n^2} \sum_{j=1}^{n} \var(Y_{ij}) &&\text{since $Y_{ij}$ are i.i.d}\\
        &= \frac{1}{n^2} \sum_{j=1}^{n} [\var(\alpha_i) + \sigma^2]\\
        &= \frac{\var(\alpha_i) + \sigma^2}{n}.
    \end{align*}
    
If $i = j$, we have 
    
\[ \var(\hali - 2\halj) = \var(- \hali) = \var(\hali) = \frac{\var(\alpha_i + \sigma^2)}{n}. \]
Otherwise if $i \neq j$, then 

\begin{align*}
    \var(\hali - 2\halj) &= \var \left(\frac{1}{n} \sum_{j=1}^{n} Y_{ij} - \frac{2}{n} \sum_{i=1}^{n} Y_{ij}  \right)\\
    &= \frac{1}{n^2} \sum_{j=1}^n \var(Y_{ij}) + \frac{4}{n^2} \sum_{j=1}^n \var(Y_{ij})\\
    &= \frac{5}{n^2}\sum_{j=1}^n \var(Y_{ij})\\
    &= \frac{5 (\var(\hali) + \sigma^2)}{n}.
\end{align*}

### c) 
We have
\begin{align*}
    \ex[S_{\Omega} - 3S_{\omega}] &= \ex[S_{\Omega}  - S_{\omega} - 2S_{\omega}]\\
    &= \ex[S_{\Omega} - S_{\omega}] - 2\ex[S_{\omega}]\\
    &= I - 1 - 2(n - 1).
\end{align*}

Similarly, 

\begin{align*}
    \ex[3S_{\Omega} - 5S_{\omega}] &= \ex[3(S_{\Omega} - S_{\omega}) - 2S_{\omega}]\\
    &= 3\ex[S_{\Omega} - S_{\omega}] - 2\ex[S_{\omega}]\\
    &= 3(I - 1) - 2(n-1).
\end{align*}

## Question 2

### a) 

Using the $\mu^{st} = 0$ parametrization, we have 
\[ \hali^{st} = \frac{1}{n_i} \sum_{j=1}^{n_i} Y_{ij} = \Bar{Y}_{i \cdot} = \hali^{tr} + \hat{\mu}^{tr}  \]
and 

\[ \hm^{st} = 0. \]

### b) 

We have 
\[ \hm^{sum} = \frac{1}{I} \sum_{i=1}^{I} \frac{1}{n_i} \sum_{j=1}^{n_i} Y_{ij} = \frac{1}{I} \sum_{i=1}^{I} \hali^{tr}  \]
    and 
\[ \hali^{sum} = \Bar{Y}_{i \cdot} - \Bar{Y}_{\cdot \cdot} = \hali^{tr} - \hm^{sum}. \]

### c) 

We have 
\[ \hm^{sum} = \frac{1}{I} \sum_{i=1}^{I} \frac{1}{n_i} \sum_{j=1}^{n_i} Y_{ij} = \frac{1}{I} \sum_{i=1}^{I} \hali^{tr}  \]
    and 
\[ \hali^{sum} = \Bar{Y}_{i \cdot} - \Bar{Y}_{\cdot \cdot} = \hali^{tr} - \hm^{sum}. \]

## Question 3

### a)

Using the formula $\hat{\gamma}_{ij} = Y_{ij.} - Y_{i..} - Y_{.j.} + Y_{...} $, we can estimate $\hat{\gamma}$. This is:
        \begin{equation*}
            \mathds{E}(\hat{\gamma}_{ij}) = \mathds{E}(Y_{ij.}) - \mathds{E}(Y_{i..}) - \mathds{E}(Y_{.j.}) + \mathds{E}(Y_{...})
            \n
        \end{equation*}
        \begin{align*}
        \ex[3S_{\Omega} - 5S_{\omega}] &= \ex[3(S_{\Omega} - S_{\omega}) - 2S_{\omega}]\\
        &= 3\ex[S_{\Omega} - S_{\omega}] - 2\ex[S_{\omega}]\\
        &= 3(I - 1) - 2(n-1).
    \end{align*}

## Question 4

### a) Using the parametrization $\mu = 0$:

``` {r, echo=TRUE}

data("iris");

Y <- iris[order(iris$Species), "Sepal.Width"];

X <- diag(3) %x% rep(1, 50);

n = 150;
I = 3;
```
Then we calculate the estimated $\hat{\beta} = (X^TX )^{-1}X^T Y$ as

``` {r, echo=TRUE}

beta = solve((t(X) %*% X)) %*% t(X) %*% Y;

```

The residual sum of squares $S_{\Omega}$ and $S_{\omega}$ of the full and reduced
models respectively are 

```{r, echo=TRUE}

s1 = norm(Y - X %*% beta, type="2")^2;
s2 = norm(Y - matrix(rep(1, n),ncol=1) * mean(Y), type="2")^2;
```
The unbiased estimator of $\sigma^2$ are $\frac{S_{\Omega}}{n - 1} =  `r s1`$
and $\frac{S_{\omega}}{n - 1} =  `r s2`$.
```{r, echo=TRUE}
unb_est = s1/(n - I);
bet_ss = s2 - s1;
bet_means = (s2 - s1)/(I);
f_val = ((s2 - s1)/(I - 1))/(s2/(n - I));

within_means = s1/(n - I);
```
The quantities needed to complete an ANOVA table are :

\begin{itemize}

\item Between groups sum of square: $S_{\omega} - S_{\Omega} = `r bet_ss`$.
\item Between groups mean square: $\frac{(S_{\omega} - S_{\Omega})/(I - 1)}{S_{\Omega}/(n - I)} = `r bet_means`$.
\item Within groups sum of square: $S_{\Omega} = `r s1`$.
\item $F$ value = `r  f_val`.

\end{itemize}

### b) We first check for the model assumptions:
The normality of residuals with expectation zero are checked using QQ-plot, 
Shapiro-Wilk test, and one-sample t-test because the true standard deviation is not
known.

```{r, echo=TRUE, out.lines=2:6}
plot.new()
res <- Y - c(rep(beta, 1, each=50));
qqnorm(res)
qqline(res)

shapiro.test(res)

t.test(res, mu=0, alternative = "greater")
```
Since the $p$-values for both test are larger than 0.05, with the mean of the residuals being
extremely close, we can say that the normality and zero mean assumptions hold. 
Next we check that $Var(e_{ij}) = \sigma^2$ using Bartlett test.

```{r, echo=TRUE}

species <- factor(c(rep(1, 50), rep(2, 50), rep(3, 50)),
                  labels=c("setosa", "versicolor", "virginica"));
data_iris <- data.frame(Y, species);

bartlett.test(Y ~ species, data=data_iris)
```
Thus, the model assumptions hold. Now we test for the mean of iris sepal width of the 
three species using the produced $F$-statistic above.


```{r, echo=TRUE}

pv <- pf(f_val, I - 1, n - I, lower.tail = FALSE);

```

The $p$-value is `r pv` $< 0.05$ so we can reject the null hypothesis
that the means are statistically the same. 

### c) 

```{r, echo=TRUE}
  
model <- aov(Y ~ species, data=data_iris);

summary(model)

```
It can be seen that the results from ANOVA agree with the final conclusion although
some quantities are a bit off. 

### d) 

```{r, echo=TRUE}
kruskal.test(Y ~ species, data=data_iris);
```
Thus, the Kruskal-Wallis test agrees with our findings since its $p$-value is smaller
than 0.05, and because we the normal distribution assumption holds, the location parameters
are the means.

## Question 5
### a) Plot of average yield per block, distinguishing between using or not using nitrogen
```{r, echo=TRUE}
utils::data(npk, package="MASS")
utils::data(npk, package="MASS")
npk_nitro = npk[npk$N == 1,]
npk_nonnitro = npk[npk$N == 0,]
bck = matrix(0,6,2)

for (i in 1:6)
  {
  bck[i,1] = mean(npk_nitro[npk_nitro$block == i,]$yield)
  bck[i,2] = mean(npk_nonnitro[npk_nonnitro$block == i,]$yield)
}

df = data.frame(bck[1,], bck[2,], bck[3,], bck[4,], bck[5,], bck[6,])
colnames(df) = c("Block 1", "Block 2", "Block 3", "Block 4", "Block 5", "Block 6")

barplot((as.matrix(df)),
        beside=TRUE,
        main = "Average yield per block \n(with and without nitrogen)")

### b) Two way ANOVA full test
```{r, echo=TRUE}
mod.full=lm(yield ~ block*N, data = npk) 
anova(mod.full)
## H0: no interaction versus H1: there is an interaction 
## H0: smaller model holds versus H1: smaller model is not true 
```
The p-values for both "block" and "N" are small enough for being significant for us. However, the value for the interaction is clearly above the significant level, so we cannot reject that the interaction between "block" and "N" does not exist (i.e. we do not have enough evidence of the existance of interaction). 

```{r, echo=TRUE}
mod.full=lm(yield ~ block, data = npk)   # full model
anova(mod.full)
```
```{r, echo=TRUE}
mod.full=lm(yield ~ block, data = npk)   # full model
anova(mod.full)
```
Note that if we conduct the one way ANOVA test with the two variables by separate, the "block" variable does not seem to be significant enough for our analysis. Moreover, the analogous test for the "N" variable outputs a p-value that indicates that the use of nitrogen is really significant.

## Question 6
```{r, echo=TRUE}

diet <- read.table("diet.txt", header = TRUE);
diet["weight.loss"] <- diet$preweight - diet$weight6weeks;

```

### a) A short summary of the data is given:

```{r, echo=TRUE}
summary(diet);

```
To further see the effects of the diets on weight loss, we use boxplots. 

```{r, echo=TRUE}

boxplot(weight.loss ~ diet, data=diet)


```
It can be seen that there are a few outliers within the samples and it may 
affect our tests later thus we shall remove them.

```{r, echo=TRUE}

Q1 <- quantile(diet$preweight, probs=c(.25, .75), na.rm = FALSE)
Q2 <- quantile(diet$weight6weeks, probs=c(.25, .75), na.rm = FALSE)

iqr_pre <- IQR(diet$preweight);
iqr_aft <- IQR(diet$weight6weeks);

diet_elim <- subset(diet, (preweight > (Q1[1] - 1.5*iqr_pre) & 
                          preweight < (Q1[2]+1.5*iqr_pre)) |
                          (weight6weeks < (Q2[1] - 1.5*iqr_aft) &
                          weight6weeks > (Q2[1] + 1.5*iqr_aft)));



boxplot(diet_elim$preweight, diet_elim$weight6weeks,
        names=c("Preweight", "Weight6weeks"),
        ylab="kg",
        main="Boxplot of Preweight and weight6weeks after removing outliers")

```


To check whether the diets affect the weight loss, we can test for statistical 
difference between $preweight$ and $weight6weeks$, if the diet does not affect 
then the mean is approximately the same and vice versa. We first use QQ-plot 
and Shapiro-Wilk test to check the normality of the samples.


```{r, echo=TRUE}

par(mfrow=(c(1,2)))

qqnorm(diet_elim$preweight, main="QQ-plot of preweight")
qqline(diet_elim$preweight)

qqnorm(diet_elim$weight6weeks, main="QQ-plot of weight6weeks")
qqline(diet_elim$weight6weeks)

shapiro.test(diet_elim$preweight)
shapiro.test(diet_elim$weight6weeks)
```
The two $p$-values are higher than 0.05 thus we can safely assume that they do
not significantly differ from normal distribution and the two samples $t$-test can
be used.

```{r, echo=TRUE, out.lines=2:6}

t.test(diet_elim$preweight, diet_elim$weight6weeks)

```

Since the resulting $p$-value from the $t$-test is smaller than 0.05, we can reject
the null hypothesis that their means are the same, i.e there is a statistical
significant difference in the means and the diets do affect the weight loss.

### b) 
To check whether any type of diet has an effect on the lost weight,
we use ANOVA to test the null hypothesis that across all three diets, the means 
of lost weights are the same.

```{r, echo=TRUE}

an_mod <- aov(weight.loss ~ diet, data=diet_elim)

summary(an_mod)
```
The $p$-value is smaller than 0.05 so we can reject the null hypothesis and say
that the diets have an effect on losing weight.

To check which diet is best for losing weight, we test for the their repspective means 
to see which has the highest means i.e expected lost weight. By definition,

$$ weight.loss = preweight - weight6weeks,  $$,
but $preweight$ and $weight6weeks$ are normally distributed so we can assume that
$weight.loss$ is also normally distributed and the $t$-test can be used. We 
check if diet 3 is more effective than 1 and 2.

```{r, echo=TRUE, out.lines=c(2:6, 13:17)}

wl1 = subset(diet_elim, diet == "1");
wl2 = subset(diet_elim, diet == "2");
wl3 = subset(diet_elim, diet == "3");

t.test(wl3$weight.loss, wl1$weight.loss, alternative = "greater")
t.test(wl3$weight.loss, wl2$weight.loss, alternative = "greater")

```
Since the $p$-values are smaller than 0.05, we reject the null hypothesis that
the means are the same so diet 3 is more effective than 1 and 2.


### c)

We use two-way anova to investigate the effect of diet, gender, and their interaction
on weight loss

```{r, echo=TRUE}

tw_aov1 <- aov(weight.loss ~ diet * gender, data=diet_elim);

summary(tw_aov1)

```
The $p$-values for gender and interaction between diet and gender are larger 
than 0.05 so there are no statistical significance for their effects on 
weight loss as opposed to diet alone.

### d)
We investigate the effect of diet and height using ANCOVA. We test the hypothesis
$H_{A}: \alpha_i = \dots = \alpha_I = 0$
```{r, echo=TRUE}

anc1 <- lm(weight.loss ~ height + diet, data=diet_elim);

anova(anc1)


```
The $p$-value is smaller than 0.05 so we reject the null hypothesis that the diet 
does not affect the weight loss. Similarly, we test for $H_{\beta}: \beta = 0$.

```{r, echo=TRUE}

anc2 <- lm(weight.loss ~ diet + height, data=diet_elim);

anova(anc2)


```
The $p$-value is larger than 0.49 so we can not reject the null hypothesis that
the height does not have an effect.
The interaction between diet and height us subsequently tested.

```{r, echo=TRUE}

anc3 <-  lm(weight.loss ~ height * diet, data=diet_elim);

anova(anc3)

```
The $p$-value for the interaction effect is larger than 0.05 so it does not bear
any statistical significant effect towards weight loss. Furthermore, for the 3 types 
of diet, the effect of height is the same because of the hypothesis 
$H_{A \beta}: \beta_i = \dots = \beta_I$ and we did not reject it.

### e)
Out of two approaches, we prefer the b) one because in d), the height and the interaction
effects statistically do not effect weight loss. Since diet is the only (tested) 
factor to have a significant effect on weight loss, we can do a simple linear regression
model.

```{r, echo=TRUE, out.lines=9:12}

lm.model <- lm(weight.loss ~ diet, data=diet_elim)

summary(lm.model)


```
So, based on the model, the lost weight of an average person only depend on their 
chosen diet and can be given as 
$$lost\_weight = 1.9 +  0.9 \times diet.$$





