---
title: "Assignment 2"
output: pdf_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
library(knitr)
library(afex)
library(performance)
library(magrittr)
library(MASS)
library(ggplot2)
library(plyr)
library(EnvStats)
hook_output <- knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$out.lines)) {
    n <- as.numeric(n)
    x <- unlist(stringr::str_split(x, "\n"))
    nx <- length(x) 
    x <- x[pmin(n,nx)]
    if(min(n) > 1)  
      x <- c(paste(options$comment, "[...]"), x)
    if(max(n) < nx) 
      x <- c(x, paste(options$comment, "[...]"))
    x <- paste(c(x, "\n"), collapse = "\n")
  }
  hook_output(x, options)
    })
```
## Question 1
```{r, echo=FALSE}

tstat = (4-2)/(1.1)^(1/2);
crit_val = qt(0.1/2, 100-3, lower.tail = FALSE);
```
### a) 

The test statistic for the hypothesis is 
\[ T = \frac{\hat{\theta_3} - 2}{\sqrt{\hat{\Sigma}_{3,3}}} = \frac{4 - 2}{\sqrt{1.1}} = `r tstat`. \]
Since $`r tstat` > t_{97, 0.95} = `r crit_val`$ so we reject $H_0$. 

### b) 

Let $\alpha=0.05$, the $95\%$ confidence interval is 
\[ \hat{\theta_3} \pm t_{97,0.975} \sqrt{\hat{\Sigma}_{3,3}} = 4 \pm 1.98 \times1.04 = [1.94, 6.05].  \]

### c) 

We have the statistic 
\[ T = \frac{f(0, \hat{\theta})}{\sqrt{\hat{v}_x^{T} \hat{\Sigma} \hat{v}}} = \frac{0.54}{0.85} = 0.63  \]
which is lower than $t_{97,0.975}$ so we can not reject $H_0$. 

### d) 

The $95\%$ confidence interval is 
\[ f(0, \hat{\theta}) \pm t_{97,0.975} \times \sqrt{\hat{v}_x^{T} \hat{\Sigma} \hat{v}} = [-1.14, 2.22]. \]

### e) 
Since the dataset with the funtion $f(x, \theta)$, we can compute the matrix $\hat{V}$ where
\[ \hat{V}_{ij}  = \partial f(x_i, \hat{\theta})/\partial \theta_j. \]
Thus, we can compute $(\hat{V}^{T} \hat{V})^{-1}$ and 
\[ \hat{\sigma}^2 = \hat{\Sigma}(\hat{V}^{T} \hat{V})  \]
where $\hat{\Sigma}$ is the given estimated covariance matrix.

## Question 2
### a) 
The estimates of $\theta$ are $\hat{\theta} = (0.81, -0.44, 1.98, 1.27)$ and 
$\hat{\sigma}^2 = \frac{S(\hat{\theta})}{n - p}$ where $S(\hat{\theta}) = RSE^2 (n - p)$
where $RSE$ is the residual standard error. Thus, we have $\hat{\sigma}^2 = RSE^2 = 0.275$.
The residual sum of squares can be recovered from the residual standard error and
the degree of freedoms because 
\[ RSE = \sqrt{\frac{RSS}{n - p}} \]
where $p$ is the number of parameters $\theta$, which in this case, is 4. 

### b) 
We first obtain the estimated covariance matrix or just the values on the diagonal of $\hat{\Sigma}$ using $x_i = \frac{3(i - 1)}{n - 1}$ 
with $n = 100$ and the partial derivatives $\partial f(x_i, \hat{\theta})/\partial \theta_j$.
Note that the partial derivatives calculations are quite straightforward so they will be omitted.
We have $\hat{\Sigma}_{1,1} = 0.114$, $\hat{\Sigma}_{2,2} = 0.028$,
$\hat{\Sigma}_{3,3} = 0.0076$, and $\hat{\Sigma}_{4,4} = 0.1104$. 
Like before, we have the test statistic
\[ T = \frac{0.8}{\sqrt{0.114}} = 2.37 > t_{96,1 - 0.05/2} = 1.984\]
so we reject $H_0$. The $95\%$ confidence interval is 
\[ \hat{\theta_1} \pm t_{96,1 - 0.05/2} \sqrt{\hat{\Sigma}_{1,1}} = 0.81 \pm 1.984 \times 0.337 = [0.141, 1.478].\]

### c) 
Likewise, we have the statistic
\[ T = \frac{\hat{\theta_4} - 1}{\sqrt{\hat{\Sigma}_{4,4}}} = 0.81 < t_{96,1 - 0.05/2} \]
so we can not reject $H_0$. Let $\alpha = 0.02$ then the $98\%$ confidence interval is 
\[  \hat{\theta_2} \pm t_{96,1 - 0.02/2} \sqrt{\hat{\Sigma}_{2,2}} = -0.44 \pm 2.36 \times 0.028 = [-0.5, -0.37]. \]

### d) 


### e) 
Let the global model $\Omega$ be the full model, i.e, $S({\hat{\theta_q}}) = \min_{\theta_q} || Y - f_{\Omega}(x, \theta_q) ||^2$, where $\theta_q \in \mathbb{R}^q$, and 
the linear submodel to be $\omega: S({\hat{\theta_p}}) = \min_{\theta_p} || Y - f_{\omega}(x, \theta_p) ||^2$
where $\theta_p \in \mathbb{R}^{p}$ with $p < q$ such that $f_{\omega}(x, \theta_p)$ is linear. 
We can test for $H_0$ that the linear submodel fits well at significance level $\alpha$
by using the test statistic 
\[ V = \frac{[S({\hat{\theta_p}}) - S({\hat{\theta_q}})]/(q - p)}{S({\hat{\theta_q}})/(n - q)}. \]
If $V > F_{q - p, n - q,1 - \alpha}$ then we reject $H_0$ that the linear submodel fits well. 


## Question 3
### a)

```{r, include=FALSE, echo=TRUE, out.lines=1:13}
N = 100
x = runif(N,0,4)
theta = c(2, 3, 1)
sigma = 0.5 #sigma_sq = 0.25
eps = rnorm(N, 0, sigma)
f <- function(x) {return(theta[1] * x + theta[2] / (theta[3] + 3*x^2))} 
y = x # simulated observations
ftrue = x # true values
for (i in 1:N) {
  y[i] = f(x[i]) + eps[i]
  ftrue[i] = f(x[i])
  }
form=as.formula(y ~ theta1*x + theta2/(theta3+3*x^2))
dfy = data.frame(y)
dfy['x'] = x
```

In this question, we need to artificcially generate our data. We will take $x_i \in [0,4]$ with $i = 1, \dots ,100$ and our function $f(x,  \bm{\theta}) = \theta_1 x + \theta_2 / (\theta_3 + 3 x^2). To our simulated response we have added an error term $\epsilon_i \sim N(0, \sigma^2)$ such that $y_i = f(x_i) + e_i$. Here, we chose $\bm{\theta} = (2, 3, 1)$ and $\sigma = 0.5$. The following plot illustrates the dataset.


```{r, include=TRUE, echo=TRUE, out.lines=1:13}
plot(x,y,
     xlab="Values for x",
     ylab="Values for y",
     main='Plotting simulated values agains true values') # plot of dataset
```

Now, we estimate the parameter $\bm{\theta}$ by means of a non-linear model. The model's summary will give us our first insights about the quality of our estimation.

```{r, include=TRUE, echo=TRUE, out.lines=1:13}
nmodel=nls(form,data=dfy,start=c(theta1=theta[1],theta2=theta[2],theta3=theta[3])) 
hat.th=coef(nmodel); hat.th ## the LSE estimates of theta's
summary(nmodel) # more informative output
```
Indeed, we can observe that our estimates seem to be quite accurate, but not to the same extent for the three dimensions of $\bm{\theta}$, yet the estimation for $\theta_2$ seems to be slightly more off with respect to the other two dimesions of $\bm{\theta}$.

If we see our estimate for the error's variance, we will see that it does give a relatively good estimation (although that obviously depends on the context of the data:

```{r, include=FALSE, echo=TRUE, out.lines=1:13}
RSS=deviance(nmodel);RSS # the same info in nmodel
y_var=RSS/(N-2) 
y_var
```
Similarly, here we have our covariance matrix using our estimates:
```{r, include=FALSE, echo=TRUE, out.lines=1:13}
cov.est=vcov(nmodel)
cov.est
```

Finally, we can feet our estimate function, i.e. $f(x, \bm{\hat{\theta}}$, in the plot.

```{r, include=FALSE, echo=TRUE, out.lines=1:13}
f=function(x,theta)return(theta[1] * x + theta[2] / (theta[3] + 3*x^2))
coef(nmodel) # estimates for theta
x=seq(from=0,to=4,length.out=N)
lines(x,f(x,coef(nmodel)),col="red")  # fitted curve
```

### b)

The 98% level confidence interval for $\bm{theta} = (\theta_1, \theta_2, \theta_3)$ can be directly computed by assymptotic normality and will yield the following intervals:

```{r, include=TRUE, echo=TRUE, out.lines=1:13}
lb=numeric(3); ub=numeric(3); # rownames(lb)=names(coef(nmodel))
for(i in 1:3) {lb[i]=coef(nmodel)[i]-qt(0.98,N-length(coef(nmodel)))*sqrt(cov.est[i,i])
      ub[i]=coef(nmodel)[i]+qt(0.98,N-length(coef(nmodel)))*sqrt(cov.est[i,i])}
ci=cbind(lb,ub); rownames(ci)=names(coef(nmodel))
ci
```
Also, we can compute confidence intervals of the same confidence level by means of the bootstrap method:

```{r, include=FALSE, echo=TRUE, out.lines=1:13}
B=1000 # 1000
par.boot=matrix(NA,B,length(coef(nmodel)))
rownames(par.boot)=paste("B",1:B,sep="")
colnames(par.boot)=names(coef(nmodel))
res.centered=resid(nmodel)-mean(resid(nmodel))

for(b in 1:B){
  # cat("b = ",b,"\n")
  # Bootstrap samples from centered residuals
  res=sample(res.centered,replace=T)
  # Calculate bootstrap values for the response
  yboot=fitted(nmodel)+res #Y*_1,...Y*_n
  # Fit model using new response and get bootstrap estimates for parameter 
  modelBoot=nls(yboot~theta1*x+theta2/(theta3+3*x^2),
                data=data.frame(yboot,x),start=list(theta1=2,theta2=3,theta3=1))
  # Store estimated (by bootstrap) parameters  
  par.boot[b,]=coef(modelBoot) #\theta*_1,..., \theta*_B
}
```
```{r, include=TRUE, echo=TRUE, out.lines=1:13}
lb.boot=2*coef(nmodel)-apply(par.boot,2,quantile,prob=0.98)
ub.boot=2*coef(nmodel)-apply(par.boot,2,quantile,prob=0.02)
cbind(lb.boot,ub.boot)
```
### c)

Let's see the expected value for $Y$ when $x=3$ in a $98\%$ confidence interval. Note that both the actual value of $f(3, \theta) and our estimate are included in the interval. This should not be surprising, yet we are looking at a relatively high confidence level.

```{r, include=TRUE, echo=TRUE, out.lines=1:13}
f3=f(3,coef(nmodel))
f3; f(3,theta)

grad<-function(x,theta){rbind(x,
                              1 / (theta[3] + 3 * x^2),
                              -theta[2] / (theta[3] + 3*x^2) / (theta[3] + 3*x^2) )}
gradvec=grad(3,coef(nmodel))
se=sqrt(t(gradvec)%*%vcov(nmodel)%*%gradvec) #0.006986284
lb=f3-qt(0.99,N-length(coef(nmodel)))*se #1.156622
ub=f3+qt(0.99,N-length(coef(nmodel)))*se #1.179728
c(lb,ub)
```
### d)

This can be done for all $x \in [0,4]$. To gain some insight, we will make use of the plot seen before but this time including the confidence intervals.

```{r, include=FALSE, echo=TRUE, out.lines=1:13}
x=seq(from=0,to=4,length.out=N)
fe=f(x,coef(nmodel)) # or: fe=predict(nmodel,newdata=data.frame(conc=x))
mygrad=grad(x,coef(nmodel)) # estimated gradients for x's from [0,4]
se<-sqrt(apply(mygrad,2,function(xx) t(xx)%*%vcov(nmodel)%*%xx))
lb<-fe-qt(0.99,N-length(coef(nmodel)))*se 
ub<-fe+qt(0.99,N-length(coef(nmodel)))*se
```
```{r, include=TRUE, echo=TRUE, out.lines=1:13}
plot(dfy['x'],f(x,theta),xlab="the x",ylab="the y") # data
lines(x,fe,t="l",lwd=0.5,col="red") # fitted curve
segments(x,lb,x,ub,col="grey") # confidence intervals
```

### e)

```{r, include=TRUE, echo=TRUE, out.lines=1:13}
U = coef(nmodel)[1] - coef(nmodel)[2]
t = U / sqrt((cov.est[1,1] + cov.est[2,2] - 2 * cov.est[1,2]^2))
pval = pt(abs(t), df = 97, lower.tail = FALSE)
```

## Question 4
### a) 

```{r, include=TRUE, echo=TRUE, out.lines=1:13}
stormer <- data.frame(stormer);
smod_lin <- lm(Wt * Time ~ Viscosity + Time, data = stormer);
summary(smod_lin)
```
Fitting the linear regression $wT = \theta_1 v + \theta_2T + (w - \theta_2)\varepsilon$
returns the estimates $\hat{\theta}_1 = 28$ and $\hat{\theta}_2 = 2$ which we will use 
for the initial values of the non-linear regression. 

```{r, include=TRUE, echo=TRUE, out.lines=1:9}
n = nrow(stormer);
p = 2;
smod.nls <- nls(Time ~ (the1 * Viscosity)/(Wt - the2),
               data = stormer, 
               start=c(the1=28, the2=2)
               );
RSS=deviance(smod.nls);
smod.evar <- round(RSS/(n - p), 2);
summary(smod.nls)
```
Applying non-linear regression, we have the estimates for $(\theta_1, \theta_2)$
as $(\hat{\theta}_1, \hat{\theta}_2) = (29.4, 2.2)$ which are very close to the 
inital values.The estimated variance of the error $\hat{\sigma}^2 = `r smod.evar`$.
To test for the validity of the model's assumptions, we use residual plot and the 
qq plot, Shapiro-Wilk test to check for the normality of the residual.

```{r, include=TRUE, echo=TRUE}
plot(fitted(smod.nls), resid(smod.nls), xlab="Fitted values", ylab="Residuals");
abline(0, 0)

qqnorm(resid(smod.nls))
qqline(resid(smod.nls))

shapiro.test(resid(smod.nls))

```
It can be seen that the pattern of the points are somewhat random so the non-linear 
model is somewhat good. Since the $p$-value of the Shapiro-Wilk test is 0.55,
which is higher than 0.05, we reject the null hypothesis that the data is not 
normally distributed. The QQ-plot also confirms the normality assumption. 

### b) 

```{r, include=TRUE, echo=FALSE}

smod.cov = vcov(smod.nls);
T <- (2.2 - 2)/(sqrt(smod.cov[2,2]));

```
Calculate the test statistic $T$ using the covariance matrix, we have that 
$T = `r T` < t_{21, 1 - 0.05/2} = 2.08$ so we can not reject $H_0$. 


### c) 
```{r, include=TRUE, echo=TRUE}

lb=numeric(2); ub=numeric(2);
for(i in 1:2) {lb[i]=coef(smod.nls)[i]-qt(0.975,n-length(coef(smod.nls)))*sqrt(smod.cov[i,i])
      ub[i]=coef(smod.nls)[i]+qt(0.975,n-length(coef(smod.nls)))*sqrt(smod.cov[i,i])}
ci=cbind(lb,ub); rownames(ci)=names(coef(smod.nls)); ci

```
The $95\%$ confidence interval for $\hat{\theta}_1$ is $[27.49, 31.3]$ and $\hat{\theta}_2$
is $[0.83, 3.6]$.

### d)
``` {r, include=TRUE, echo=TRUE}

grad<-function(v,w,the){rbind(v/(w - the[2]), the[1]*v/(w - the[2])^2)};
gradvec <- grad(100, 60, coef(smod.nls));

se=sqrt(t(gradvec)%*%vcov(smod.nls)%*%gradvec);
f <- function(v, w, the) { the[1]*v/(w - the[2]) };
f4 <- f(100, 60, coef(smod.nls))

lb=f4-qt(0.05/2,n-length(coef(smod.nls)), lower.tail=FALSE)*se
ub=f4+qt(0.05/2,n-length(coef(smod.nls)), lower.tail=FALSE)*se

c(lb, ub)

```
The $95\%$ confidence interval for the expected value of $T$ is therefore $[48.65, 53.1]$.

### e) 
```{r, include=TRUE, echo=TRUE}

form2 <- as.formula(Time ~ (the1 * Viscosity)/(Wt));
smod.nls2 <- nls(form2, data=stormer, start=c(the1=28));
anova(smod.nls, smod.nls2)

```
Using ANOVA, the reduced model gives a worse fit than the full one since its 
residual sum of squares is $1210 > 825.05$. We also calculate the $V$ statistic 
to compare against the $F$-distribution.

```{r, include=TRUE, echo=TRUE}

SSq <- deviance(smod.nls);
SSp <- deviance(smod.nls2);

n <- length(resid(smod.nls)); 
q<- length(coef(smod.nls)); 
p <- length(coef(smod.nls2)) 

fstat <- ((SSp-SSq)/(q-p))/(SSq/(n-q));
pval <- 1 - pf(fstat, q-p, n-p);
```
The obtained $F$-statistic is $`r fstat` > F_{1, 21, 0.95} = 4.32$  
and its $p$-value is $`r pval` < 0.05$ so we reject the null hypothesis $H_0$ that 
the smaller model $\omega$ is appropriate. 

```{r, include=TRUE, echo=TRUE}

AIC(smod.nls)
AIC(smod.nls2)


```
The AIC of the full model is $153.6$ while the AIC for the smaller one is $160.4$. Clearly,
$153 < 160$ so the full model fits better. 

















